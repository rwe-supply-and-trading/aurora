{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a6cb42",
   "metadata": {},
   "source": [
    "# Making predictions using Aurora Wave Model\n",
    "\n",
    "In this example, we need to download both WeatherBench HREST0 data, and HRES-WAM data, that is data from ECMWF's ocean wave model called HRES-WAM. \n",
    "\n",
    "Running this notebook requires additional Python packages. You can install these as follows:\n",
    "\n",
    "```\n",
    "pip install ecmwf-api-client matplotlib gcsfs cdsapi zarr cfgrib\n",
    "```\n",
    "\n",
    "## Downloading the HRES-WAM Data from the MARS archive\n",
    "\n",
    "To begin with, register an account with [ECMWF](https://api.ecmwf.int/v1/key) and create `$HOME/.ecmwfapirc` with the following content:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"url\"   : \"https://api.ecmwf.int/v1\",\n",
    "    \"key\"   : <API key>,\n",
    "    \"email\" : <email>\n",
    "}\n",
    "```\n",
    "\n",
    "You can find your API key on your account page.\n",
    "\n",
    "\n",
    "We now download the HRES-WAM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6db9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import ecmwfapi\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up the variables we want to download\n",
    "variables: dict[str, str] = {\n",
    "    \"swh\": \"140229\",  # Significant wave height\n",
    "    \"pp1d\": \"140231\",  # Peak wave period\n",
    "    \"mwp\": \"140232\",  # Mean wave period\n",
    "    \"mwd\": \"140230\",  # Mean wave direction\n",
    "    \"shww\": \"140234\",  # Significant height of wind waves\n",
    "    \"mdww\": \"140235\",  # Mean direction of wind waves\n",
    "    \"mpww\": \"140236\",  # Mean period of wind waves\n",
    "    \"shts\": \"140237\",  # Significant height of total swell\n",
    "    \"mdts\": \"140238\",  # Mean direction of total swell\n",
    "    \"mpts\": \"140239\",  # Mean period of total swell\n",
    "    \"swh1\": \"140121\",  # Significant wave height of first swell\n",
    "    \"mwd1\": \"140122\",  # Mean direction of first swell\n",
    "    \"mwp1\": \"140123\",  # Mean period of first swell\n",
    "    \"swh2\": \"140124\",  # Significant wave height of second swell\n",
    "    \"mwd2\": \"140125\",  # Mean wave direction of second swell\n",
    "    \"mwp2\": \"140126\",  # Mean wave period of second swell\n",
    "    \"dwi\": \"140249\",  # 10m wind direction\n",
    "    \"wind\": \"140245\",  # 10m wind speed\n",
    "}\n",
    "# Convert to form required by MARS.\n",
    "for k, v in variables.items():\n",
    "    assert len(v) == 6\n",
    "    variables[k] = v[3:6] + \".\" + v[0:3]\n",
    "\n",
    "c = ecmwfapi.ECMWFService(\"mars\")\n",
    "first_day = datetime.strptime(\"2022-09-16\", \"%Y-%m-%d\")\n",
    "last_day = datetime.strptime(\"2022-09-16\", \"%Y-%m-%d\")\n",
    "output = download_path / \"2022-09\"\n",
    "output.mkdir(exist_ok=True, parents=True)\n",
    "filename = \"2022-09-16.grib\"\n",
    "target = output / filename\n",
    "\n",
    "if not (output / \"2022-09-16.grib\").exists():\n",
    "    c.execute(\n",
    "        f\"\"\"\n",
    "        request,\n",
    "            class=od,\n",
    "            date={first_day.strftime(\"%Y-%m-%d\")}/to/{last_day.strftime(\"%Y-%m-%d\")},\n",
    "            domain=g,\n",
    "            expver=1,\n",
    "            param={\"/\".join(variables.values())},\n",
    "            stream=wave,\n",
    "            time=00:00:00/06:00:00/12:00:00/18:00:00,\n",
    "            grid=0.25/0.25,\n",
    "            type=an,\n",
    "            target=\"{filename}\"\n",
    "    \"\"\",\n",
    "        str(target),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c568d",
   "metadata": {},
   "source": [
    "## Downloading the HREST0 data from WeatherBench\n",
    "\n",
    "We need to download the corresponding data for the 16th of September 2022 from [WeatherBench2](https://weatherbench2.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# We will download from Google Cloud.\n",
    "url = \"gs://weatherbench2/datasets/hres_t0/2016-2022-6h-1440x721.zarr\"\n",
    "ds = xr.open_zarr(fsspec.get_mapper(url), chunks=None)\n",
    "\n",
    "# Day to download. This will download all times for that day.\n",
    "day = \"2022-09-16\"\n",
    "\n",
    "# Download the surface-level variables. We write the downloaded data to another file to cache.\n",
    "if not (download_path / f\"{day}-waves-surface-level.nc\").exists():\n",
    "    surface_vars = [\n",
    "        \"10m_u_component_of_wind\",\n",
    "        \"10m_v_component_of_wind\",\n",
    "        \"2m_temperature\",\n",
    "        \"mean_sea_level_pressure\",\n",
    "    ]\n",
    "    ds_surf = ds[surface_vars].sel(time=day).compute()\n",
    "    ds_surf.to_netcdf(str(download_path / f\"{day}-waves-surface-level.nc\"))\n",
    "print(\"Surface-level variables downloaded!\")\n",
    "\n",
    "# Download the atmospheric variables. We write the downloaded data to another file to cache.\n",
    "if not (download_path / f\"{day}-waves-atmospheric.nc\").exists():\n",
    "    atmos_vars = [\n",
    "        \"temperature\",\n",
    "        \"u_component_of_wind\",\n",
    "        \"v_component_of_wind\",\n",
    "        \"specific_humidity\",\n",
    "        \"geopotential\",\n",
    "    ]\n",
    "    ds_atmos = ds[atmos_vars].sel(time=day).compute()\n",
    "    ds_atmos.to_netcdf(str(download_path / f\"{day}-waves-atmospheric.nc\"))\n",
    "print(\"Atmos-level variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bf3f3",
   "metadata": {},
   "source": [
    "## Downloading the necessary static variables\n",
    "\n",
    "For this model, we also need to include static variables. We have made these available on [HuggingFace](https://huggingface.co/microsoft/aurora). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# from pathlib import Path\n",
    "\n",
    "# import cdsapi\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# # Data will be downloaded here.\n",
    "# download_path = Path(\"~/downloads\")\n",
    "\n",
    "# download_path = download_path.expanduser()\n",
    "# download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Download the static variables from HuggingFace.\n",
    "# static_path = hf_hub_download(\n",
    "#     repo_id=\"microsoft/aurora\",\n",
    "#     filename=\"aurora-0.25-wave-static.pickle\",\n",
    "# )\n",
    "# print(\"Static variables downloaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ec640",
   "metadata": {},
   "source": [
    "## Preparing a Batch\n",
    "\n",
    "We convert the downloaded data to an `aurora.Batch`, which is what the model requires. In this case, the data is a combination of HREST0 data, and HRES-WAM data to capture ocean-wave variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "# TODO: Remove this and uncomment the section to download from HuggingFace.\n",
    "static_path = \"/home/meganstanley/aurora/aurora-0.25-wave-static.pickle\"\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars = pickle.load(f)\n",
    "\n",
    "surf_vars_ds = xr.open_dataset(\n",
    "    download_path / \"2022-09-16-waves-surface-level.nc\", engine=\"netcdf4\", decode_timedelta=True\n",
    ")\n",
    "atmos_vars_ds = xr.open_dataset(\n",
    "    download_path / \"2022-09-16-waves-atmospheric.nc\", engine=\"netcdf4\", decode_timedelta=True\n",
    ")\n",
    "hres_wam_vars_ds = xr.open_dataset(\n",
    "    download_path / \"2022-09\" / \"2022-09-16.grib\", engine=\"cfgrib\", backend_kwargs={\"indexpath\": \"\"}\n",
    ")\n",
    "\n",
    "\n",
    "def _prepare_hres(x: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Prepare a variable.\n",
    "\n",
    "    This does the following things:\n",
    "    * Select the first two time steps: 00:00 and 06:00.\n",
    "    * Insert an empty batch dimension with `[None]`.\n",
    "    * Flip along the latitude axis to ensure that the latitudes are decreasing.\n",
    "    * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "    * Convert to PyTorch.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x[:2][None][..., ::-1, :].copy())\n",
    "\n",
    "\n",
    "def _prepare_hres_wam(x: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Prepare a variable.\n",
    "\n",
    "    This does the following things:\n",
    "    * Select the first two time steps: 00:00 and 06:00.\n",
    "    * Insert an empty batch dimension with `[None]`.\n",
    "    * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "    * Convert to PyTorch.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x[:2][None][...].copy())\n",
    "\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # `[None]` inserts a batch dimension of size one.\n",
    "        \"2t\": _prepare_hres(surf_vars_ds[\"2m_temperature\"].values),\n",
    "        \"10u\": _prepare_hres(surf_vars_ds[\"10m_u_component_of_wind\"].values),\n",
    "        \"10v\": _prepare_hres(surf_vars_ds[\"10m_v_component_of_wind\"].values),\n",
    "        \"msl\": _prepare_hres(surf_vars_ds[\"mean_sea_level_pressure\"].values),\n",
    "        \"swh\": _prepare_hres_wam(hres_wam_vars_ds[\"swh\"].values),\n",
    "        \"mwd\": _prepare_hres_wam(hres_wam_vars_ds[\"mwd\"].values),\n",
    "        \"mwp\": _prepare_hres_wam(hres_wam_vars_ds[\"mwp\"].values),\n",
    "        \"pp1d\": _prepare_hres_wam(hres_wam_vars_ds[\"pp1d\"].values),\n",
    "        \"shww\": _prepare_hres_wam(hres_wam_vars_ds[\"shww\"].values),\n",
    "        \"mdww\": _prepare_hres_wam(hres_wam_vars_ds[\"mdww\"].values),\n",
    "        \"mpww\": _prepare_hres_wam(hres_wam_vars_ds[\"mpww\"].values),\n",
    "        \"shts\": _prepare_hres_wam(hres_wam_vars_ds[\"shts\"].values),\n",
    "        \"mdts\": _prepare_hres_wam(hres_wam_vars_ds[\"mdts\"].values),\n",
    "        \"swh1\": _prepare_hres_wam(hres_wam_vars_ds[\"swh1\"].values),\n",
    "        \"mwd1\": _prepare_hres_wam(hres_wam_vars_ds[\"mwd1\"].values),\n",
    "        \"mwp1\": _prepare_hres_wam(hres_wam_vars_ds[\"mwp1\"].values),\n",
    "        \"swh2\": _prepare_hres_wam(hres_wam_vars_ds[\"swh2\"].values),\n",
    "        \"mwd2\": _prepare_hres_wam(hres_wam_vars_ds[\"mwd2\"].values),\n",
    "        \"mwp2\": _prepare_hres_wam(hres_wam_vars_ds[\"mwp2\"].values),\n",
    "        \"wind\": _prepare_hres_wam(hres_wam_vars_ds[\"wind\"].values),\n",
    "        \"dwi\": _prepare_hres_wam(hres_wam_vars_ds[\"dwi\"].values),\n",
    "    },\n",
    "    static_vars={\n",
    "        \"z\": torch.from_numpy(static_vars[\"z\"]),\n",
    "        \"slt\": torch.from_numpy(static_vars[\"slt\"]),\n",
    "        \"lsm\": torch.from_numpy(static_vars[\"lsm\"]),\n",
    "        \"wmb\": torch.from_numpy(static_vars[\"wmb\"]),\n",
    "        \"lat_mask\": torch.from_numpy(static_vars[\"lat_mask\"]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": _prepare_hres(atmos_vars_ds[\"temperature\"].values),\n",
    "        \"u\": _prepare_hres(atmos_vars_ds[\"u_component_of_wind\"].values),\n",
    "        \"v\": _prepare_hres(atmos_vars_ds[\"v_component_of_wind\"].values),\n",
    "        \"q\": _prepare_hres(atmos_vars_ds[\"specific_humidity\"].values),\n",
    "        \"z\": _prepare_hres(atmos_vars_ds[\"geopotential\"].values),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        # Flip the latitudes! We need to copy because converting to PyTorch, because the\n",
    "        # data must be contiguous.\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values[::-1].copy()),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element. Select element 1, corresponding to time\n",
    "        # 06:00.\n",
    "        time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400ba1",
   "metadata": {},
   "source": [
    "## Loading and Running the Model\n",
    "\n",
    "Finally, we are ready to load and run the model and visualise the predictions. We perform a roll-out for two steps, which produces predictions for hours 12:00 and 18:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bec0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import AuroraWave, rollout\n",
    "\n",
    "model = AuroraWave()\n",
    "# TODO: Put the actual HuggingFace name in here\n",
    "# TODO: the model needs to be saved as state_dict only\n",
    "# d = torch.load(\"\",\n",
    "#                weights_only=True)\n",
    "# model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-finetuned.ckpt\")\n",
    "model.load_checkpoint_local(\"\")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=2)]\n",
    "\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 6.5))\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    pred = preds[i]\n",
    "\n",
    "    ax[i, 0].imshow(pred.surf_vars[\"2t\"][0, 0].numpy() - 273.15, vmin=-50, vmax=50)\n",
    "    ax[i, 0].set_ylabel(str(pred.metadata.time[0]))\n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"Aurora Prediction\")\n",
    "    ax[i, 0].set_xticks([])\n",
    "    ax[i, 0].set_yticks([])\n",
    "\n",
    "    ref = surf_vars_ds[\"2m_temperature\"][2 + i].values[::-1, :]\n",
    "    ax[i, 1].imshow(ref - 273.15, vmin=-50, vmax=50)\n",
    "    if i == 0:\n",
    "        ax[i, 1].set_title(\"HRES T0\")\n",
    "    ax[i, 1].set_xticks([])\n",
    "    ax[i, 1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
